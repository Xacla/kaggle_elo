{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02_15_19_24\n",
      "/home/tomita/kaggle/kaggle_elo/Models\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "import gc \n",
    "import time\n",
    "import optuna\n",
    "import sklearn.metrics\n",
    "from datetime import datetime \n",
    "\n",
    "plt.style.use('ggplot') # Lets make our plots pretty\n",
    "\n",
    "path = os.getcwd()\n",
    "\n",
    "n=datetime.now().strftime(\"%m_%d_%H_%M\")\n",
    "print(str(n))\n",
    "print(path)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(201917,)\n"
     ]
    }
   ],
   "source": [
    "# Read in the dataframes\n",
    "train = pd.read_csv('../input/train_1.csv')\n",
    "test = pd.read_csv('../input/test_1.csv')\n",
    "\n",
    "#print(train.columns)\n",
    "\n",
    "target = train['target']\n",
    "train_true=np.array(train['target'])\n",
    "print(train_true.shape)\n",
    "                    \n",
    "del train['target']\n",
    "del train['outliers']\n",
    "#del train['outliners']\n",
    "features = [c for c in train.columns if c not in ['card_id', 'first_active_month']]\n",
    "categorical_feats = [c for c in features if 'feature_' in c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kfold_lightgbm(trial):\n",
    "    FEATS_EXCLUDED = ['first_active_month', 'target', 'card_id', 'outliers',\n",
    "                  'hist_purchase_date_max', 'hist_purchase_date_min', 'hist_card_id_size',\n",
    "                  'new_purchase_date_max', 'new_purchase_date_min', 'new_card_id_size',\n",
    "                  'OOF_PRED', 'month_0','outliers']\n",
    "    seed=20190208\n",
    "        \n",
    "    # params optimized by optuna\n",
    "    learning_rate_tuna = trial.suggest_uniform('learning_rate', 0, 1.0)\n",
    "    toprate_tuna = trial.suggest_uniform('top_rate', 0, 1.0)\n",
    "    num_leaves_tuna = trial.suggest_uniform('learning_rate', 0, 1.0)\n",
    "    min_child_weight_tuna = trial.suggest_int('min_child_weight', 5, 500)\n",
    "    other_rate_tuna=trial.suggest_uniform('other_rate', 0.0, 1.0)\n",
    "    num_leaves_tuna=trial.suggest_int('num_leaves', 5, 1000)\n",
    "    min_gain_split_tuna=trial.suggest_uniform('min_gain_split', 5, 500)\n",
    "    reg_lambda_tuna=trial.suggest_uniform('reg_lambda', 5, 500)\n",
    "    subsample_tuna = trial.suggest_uniform('sub_sample', 0, 1.0)\n",
    "    reg_alpha_tuna=trial.suggest_uniform('sub_sample', 0, 20)\n",
    "    colsample_bytree_tuna = trial.suggest_uniform('colsample_bytree_tuna', 0, 1.0)\n",
    "    max_depth= trial.suggest_int('max_depth', 5, 100)\n",
    "    \n",
    "    param ={'task': 'train',\n",
    "            'boosting': 'dart',\n",
    "            'objective': 'regression',\n",
    "            'metric': 'rmse',\n",
    "            'learning_rate': learning_rate_tuna ,\n",
    "            'subsample': subsample_tuna,\n",
    "            'max_depth': max_depth,\n",
    "            'top_rate': toprate_tuna ,\n",
    "            'num_leaves': num_leaves_tuna,\n",
    "            'min_child_weight': min_child_weight_tuna,\n",
    "            'other_rate': other_rate_tuna,\n",
    "            'reg_alpha': reg_alpha_tuna,\n",
    "            'colsample_bytree':colsample_bytree_tuna  ,\n",
    "            'min_split_gain': min_gain_split_tuna,\n",
    "            'reg_lambda': reg_lambda_tuna,\n",
    "            'min_data_in_leaf': 21,\n",
    "            'verbose': -1,\n",
    "            'seed':seed,\n",
    "            'bagging_seed':seed,\n",
    "            'drop_seed':seed,\n",
    "            'max_bin':255,\n",
    "            'device':'gpu'\n",
    "            }\n",
    "\n",
    "    # Create arrays and dataframes to store results\n",
    "    folds = KFold(n_splits=5, shuffle=True, random_state=15)\n",
    "    oof = np.zeros(len(train))\n",
    "    predictions = np.zeros(len(test))\n",
    "    train_prdictions = np.zeros(train.shape[0])\n",
    "    start = time.time()\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    \n",
    "    # k-fold\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)):\n",
    "        print(\"fold n°{}\".format(fold_))\n",
    "        trn_data = lgb.Dataset(train.iloc[trn_idx][features], label=target.iloc[trn_idx], categorical_feature=categorical_feats)\n",
    "        val_data = lgb.Dataset(train.iloc[val_idx][features], label=target.iloc[val_idx], categorical_feature=categorical_feats)\n",
    "\n",
    "        num_round = 10000\n",
    "        clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds = 200)\n",
    "        oof[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n",
    "    \n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = features\n",
    "        fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "        fold_importance_df[\"fold\"] = fold_ + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "        train_prdictions += clf.predict(train[features], num_iteration=clf.best_iteration) / folds.n_splits\n",
    "        predictions += clf.predict(test[features], num_iteration=clf.best_iteration) / folds.n_splits\n",
    "\n",
    "    n=datetime.now().strftime(\"%m_%d_%H_%M_%S\")\n",
    "    feature_importance_df.to_csv(\"../output/feature_importance.csv\")\n",
    "    sub_df = pd.read_csv(\"../input/sample_submission.csv\",engine='python')\n",
    "    sub_df[\"target\"] = predictions\n",
    "    sub_df.to_csv(\"../output/submit_lgb\"+str(n)+\"_optuna.csv\", index=False)\n",
    "    error_train = sklearn.metrics.mean_absolute_error(train_true,train_prdictions)\n",
    "\n",
    "    return error_train \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomita/.pyenv/versions/anaconda3-5.3.1/envs/py36/lib/python3.6/site-packages/lightgbm/basic.py:1186: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "/home/tomita/.pyenv/versions/anaconda3-5.3.1/envs/py36/lib/python3.6/site-packages/lightgbm/basic.py:752: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's rmse: 3.69927\tvalid_1's rmse: 3.73597\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study()\n",
    "study.optimize(kfold_lightgbm, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_df=pd.read_csv(\"../output/feature_importance.csv\")\n",
    "cols = (feature_importance_df[[\"feature\", \"importance\"]]\n",
    "        .groupby(\"feature\")\n",
    "        .mean()\n",
    "        .sort_values(by=\"importance\", ascending=False)[:20].index)\n",
    "\n",
    "best_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n",
    "\n",
    "plt.figure(figsize=(14,25))\n",
    "sns.barplot(x=\"importance\",\n",
    "            y=\"feature\",\n",
    "            data=best_features.sort_values(by=\"importance\",\n",
    "                                           ascending=False))\n",
    "plt.title('LightGBM Features (avg over folds)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('lgbm_importances.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n=10\n",
    "sub_df = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "sub_df[\"target\"] = predictions\n",
    "sub_df.to_csv(\"../output/submit_lgb\"+str(n)+\"_optuna.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

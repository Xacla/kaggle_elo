{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Naoki Tomita\\Anaconda3\\envs\\kaggle\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Naoki Tomita\\Desktop\\kaggle_elo\\Models\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "import gc \n",
    "import time\n",
    "import optuna\n",
    "import sklearn.metrics\n",
    "\n",
    "plt.style.use('ggplot') # Lets make our plots pretty\n",
    "\n",
    "path = os.getcwd()\n",
    "\n",
    "print(path)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(201917,)\n"
     ]
    }
   ],
   "source": [
    "# Read in the dataframes\n",
    "train = pd.read_csv('../input/train_1.csv')\n",
    "test = pd.read_csv('../input/test_1.csv')\n",
    "\n",
    "#print(train.columns)\n",
    "\n",
    "target = train['target']\n",
    "train_true=np.array(train['target'])\n",
    "print(train_true.shape)\n",
    "                    \n",
    "del train['target']\n",
    "del train['outliers']\n",
    "#del train['outliners']\n",
    "features = [c for c in train.columns if c not in ['card_id', 'first_active_month']]\n",
    "categorical_feats = [c for c in features if 'feature_' in c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kfold_lightgbm(trial):\n",
    "    FEATS_EXCLUDED = ['first_active_month', 'target', 'card_id', 'outliers',\n",
    "                  'hist_purchase_date_max', 'hist_purchase_date_min', 'hist_card_id_size',\n",
    "                  'new_purchase_date_max', 'new_purchase_date_min', 'new_card_id_size',\n",
    "                  'OOF_PRED', 'month_0','outliers']\n",
    "    seed=20190208\n",
    "        \n",
    "    # params optimized by optuna\n",
    "    learning_rate_tuna = trial.suggest_uniform('learning_rate', 0, 1.0)\n",
    "    toprate_tuna = trial.suggest_uniform('top_rate', 0, 1.0)\n",
    "    num_leaves_tuna = trial.suggest_uniform('learning_rate', 0, 1.0)\n",
    "    min_child_weight_tuna = trial.suggest_int('min_child_weight', 5, 500)\n",
    "    other_rate_tuna=trial.suggest_uniform('other_rate', 0.0, 1.0)\n",
    "    num_leaves_tuna=trial.suggest_int('num_leaves', 5, 1000)\n",
    "    min_gain_split_tuna=trial.suggest_uniform('min_gain_split', 5, 500)\n",
    "    reg_lambda_tuna=trial.suggest_uniform('reg_lambda', 5, 500)\n",
    "        \n",
    "    param ={'task': 'train',\n",
    "            'boosting': 'gbdt',\n",
    "            'objective': 'dart',\n",
    "            'metric': 'rmse',\n",
    "            'learning_rate': learning_rate_tuna ,\n",
    "            'subsample': 0.9855232997390695,\n",
    "            'max_depth': 7,\n",
    "            'top_rate': toprate_tuna ,\n",
    "            'num_leaves': num_leaves_tuna,\n",
    "            'min_child_weight': min_child_weight_tuna,\n",
    "            'other_rate': other_rate_tuna,\n",
    "            'reg_alpha': 9.677537745007898,\n",
    "            'colsample_bytree': 0.5665320670155495,\n",
    "            'min_split_gain': min_gain_split_tuna,\n",
    "            'reg_lambda': reg_lambda_tuna,\n",
    "            'min_data_in_leaf': 21,\n",
    "            'verbose': -1,\n",
    "            'seed':seed,\n",
    "            'bagging_seed':seed,\n",
    "            'drop_seed':seed,\n",
    "            'max_bin':255,\n",
    "            'device':'gpu'\n",
    "            }\n",
    "\n",
    "    # Create arrays and dataframes to store results\n",
    "    folds = KFold(n_splits=5, shuffle=True, random_state=15)\n",
    "    oof = np.zeros(len(train))\n",
    "    predictions = np.zeros(len(test))\n",
    "    train_prdictions = np.zeros(train.shape[0])\n",
    "    start = time.time()\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    \n",
    "    # k-fold\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)):\n",
    "        print(\"fold n°{}\".format(fold_))\n",
    "        trn_data = lgb.Dataset(train.iloc[trn_idx][features], label=target.iloc[trn_idx], categorical_feature=categorical_feats)\n",
    "        val_data = lgb.Dataset(train.iloc[val_idx][features], label=target.iloc[val_idx], categorical_feature=categorical_feats)\n",
    "\n",
    "        num_round = 10000\n",
    "        clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds = 200)\n",
    "        oof[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n",
    "    \n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = features\n",
    "        fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "        fold_importance_df[\"fold\"] = fold_ + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "        train_prdictions += clf.predict(train[features], num_iteration=clf.best_iteration) / folds.n_splits\n",
    "        predictions += clf.predict(test[features], num_iteration=clf.best_iteration) / folds.n_splits\n",
    "\n",
    "    feature_importance_df.to_csv(\"../output/feature_importance.csv\")\n",
    "    n=2013\n",
    "    sub_df = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "    sub_df[\"target\"] = predictions\n",
    "    sub_df.to_csv(\"../output/submit_lgb\"+str(n)+\"_optuna.csv\", index=False)\n",
    "    error_train = sklearn.metrics.mean_absolute_error(train_true,train_prdictions)\n",
    "\n",
    "    return 1-error_train \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Naoki Tomita\\Anaconda3\\envs\\kaggle\\lib\\site-packages\\lightgbm\\basic.py:1194: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\Naoki Tomita\\Anaconda3\\envs\\kaggle\\lib\\site-packages\\lightgbm\\basic.py:753: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's rmse: 3.68421\tvalid_1's rmse: 3.72518\n",
      "[200]\ttraining's rmse: 3.68421\tvalid_1's rmse: 3.72518\n",
      "Early stopping, best iteration is:\n",
      "[6]\ttraining's rmse: 3.68421\tvalid_1's rmse: 3.72518\n",
      "fold n°1\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's rmse: 3.69471\tvalid_1's rmse: 3.65342\n",
      "[200]\ttraining's rmse: 3.69471\tvalid_1's rmse: 3.65342\n",
      "Early stopping, best iteration is:\n",
      "[9]\ttraining's rmse: 3.69471\tvalid_1's rmse: 3.65342\n",
      "fold n°2\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's rmse: 3.69894\tvalid_1's rmse: 3.64379\n",
      "[200]\ttraining's rmse: 3.69894\tvalid_1's rmse: 3.64379\n",
      "Early stopping, best iteration is:\n",
      "[8]\ttraining's rmse: 3.69894\tvalid_1's rmse: 3.64379\n",
      "fold n°3\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's rmse: 3.65401\tvalid_1's rmse: 3.84641\n",
      "[200]\ttraining's rmse: 3.65401\tvalid_1's rmse: 3.84641\n",
      "Early stopping, best iteration is:\n",
      "[7]\ttraining's rmse: 3.65401\tvalid_1's rmse: 3.84641\n",
      "fold n°4\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's rmse: 3.70287\tvalid_1's rmse: 3.65922\n",
      "[200]\ttraining's rmse: 3.70287\tvalid_1's rmse: 3.65922\n",
      "Early stopping, best iteration is:\n",
      "[7]\ttraining's rmse: 3.70287\tvalid_1's rmse: 3.65922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-02-13 15:42:48,297] Finished a trial resulted in value: -0.575080548679407. Current best value is -0.575080548679407 with parameters: {'learning_rate': 0.9183840729576009, 'top_rate': 0.9654469213832032, 'min_child_weight': 350, 'other_rate': 0.2002107566847522, 'num_leaves': 606, 'min_gain_split': 455.2351871265196, 'reg_lambda': 362.09404527172865}.\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study()\n",
    "study.optimize(kfold_lightgbm, n_trials=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Naoki Tomita\\Anaconda3\\envs\\kaggle\\lib\\site-packages\\scipy\\stats\\stats.py:1633: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    }
   ],
   "source": [
    "feature_importance_df=pd.read_csv(\"../output/feature_importance.csv\")\n",
    "cols = (feature_importance_df[[\"feature\", \"importance\"]]\n",
    "        .groupby(\"feature\")\n",
    "        .mean()\n",
    "        .sort_values(by=\"importance\", ascending=False)[:20].index)\n",
    "\n",
    "best_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n",
    "\n",
    "plt.figure(figsize=(14,25))\n",
    "sns.barplot(x=\"importance\",\n",
    "            y=\"feature\",\n",
    "            data=best_features.sort_values(by=\"importance\",\n",
    "                                           ascending=False))\n",
    "plt.title('LightGBM Features (avg over folds)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('lgbm_importances.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.9183840729576009,\n",
       " 'min_child_weight': 350,\n",
       " 'min_gain_split': 455.2351871265196,\n",
       " 'num_leaves': 606,\n",
       " 'other_rate': 0.2002107566847522,\n",
       " 'reg_lambda': 362.09404527172865,\n",
       " 'top_rate': 0.9654469213832032}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=10\n",
    "sub_df = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "sub_df[\"target\"] = predictions\n",
    "sub_df.to_csv(\"../output/submit_lgb\"+str(n)+\"_optuna.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
